{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05d9e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Dict\n",
    "import json\n",
    "import requests\n",
    "import tarfile\n",
    "import io\n",
    "\n",
    "import argparse\n",
    "from functools import partial\n",
    "from typing import Any\n",
    "\n",
    "from nemo_curator.download.doc_builder import (\n",
    "    DocumentDownloader,\n",
    "    DocumentExtractor,\n",
    "    DocumentIterator,\n",
    ")\n",
    "\n",
    "import nemo_curator as nc\n",
    "from nemo_curator import ScoreFilter, Sequential\n",
    "from nemo_curator.filters import RepeatingTopNGramsFilter, WordCountFilter\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.modifiers.pii_modifier import PiiModifier\n",
    "from nemo_curator.modifiers.unicode_reformatter import UnicodeReformatter\n",
    "from nemo_curator.modules.modify import Modify\n",
    "from nemo_curator.utils.distributed_utils import get_client\n",
    "from nemo_curator.utils.script_utils import add_distributed_args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8d2bbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LivedoorCorpusDownloader(DocumentDownloader):\n",
    "    def __init__(self, download_dir: str):\n",
    "        super().__init__()\n",
    "\n",
    "        if not os.path.isdir(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "\n",
    "        self._download_dir = download_dir\n",
    "        print(\"Download directory: \", self._download_dir)\n",
    "\n",
    "    def download(self, url: str) -> str:\n",
    "        filename = os.path.basename(url)\n",
    "        output_file = os.path.join(self._download_dir, filename)\n",
    "\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"File '{output_file}' already exists, skipping download.\")\n",
    "            return output_file\n",
    "\n",
    "        print(f\"Downloading dataset from '{url}'...\")\n",
    "        response = requests.get(url)\n",
    "\n",
    "        with tarfile.open(fileobj=io.BytesIO(response.content)) as tarf:\n",
    "            tarf.extractall(self._download_dir, members=tarf)\n",
    "\n",
    "        return output_file\n",
    "\n",
    "\n",
    "class LivedoorCorpusIterator(DocumentIterator):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._counter = -1\n",
    "\n",
    "\n",
    "    def iterate(self, file_path):\n",
    "        self._counter = -1\n",
    "        file_name = os.path.basename(file_path)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            \n",
    "            content = ''.join(lines[3:]).strip()\n",
    "            if \"関連リンク\" in content:\n",
    "                content_split = content.split(\"関連リンク\",1)\n",
    "                content = content_split[0].strip(\"■\").strip()\n",
    "                related_link = content_split[1]\n",
    "            \n",
    "            else:\n",
    "                related_link = \"\"\n",
    "\n",
    "            self._counter += 1\n",
    "            meta = {\n",
    "                \"filename\": file_name,\n",
    "            }\n",
    "            content = {\n",
    "                \"url\": lines[0].strip(),\n",
    "                \"date\": lines[1].strip(),\n",
    "                \"title\":lines[2].strip(),\n",
    "                \"content\":content,\n",
    "                \"related link\":related_link\n",
    "            }\n",
    "            \n",
    "            #print(content)\n",
    "            \n",
    "\n",
    "            record = {**meta, **content}\n",
    "            return record\n",
    "\n",
    "def download_and_convert_to_jsonl() -> str:\n",
    "    \"\"\"\n",
    "    Downloads the emails dataset and converts it to JSONL format.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the JSONL file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Download the dataset in raw format and convert it to JSONL.\n",
    "    downloader =  LivedoorCorpusDownloader(DATA_DIR)\n",
    "    output_path = os.path.join(DATA_DIR, \"livedoor.jsonl\")\n",
    "    raw_fp = downloader.download(DATASET_URL)\n",
    "    \n",
    "    child_dir = DATA_DIR + \"/text/\"\n",
    "    \n",
    "    directories = [d for d in os.listdir(child_dir) if d not in [\"CHANGES.txt\", \"README.txt\"]]\n",
    "    iterator = LivedoorCorpusIterator()\n",
    "\n",
    "    \n",
    "    # Parse the raw data and write it to a JSONL file.\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for directory in directories:\n",
    "            files = [f for f in os.listdir(child_dir+directory) if f not in [\"LICENSE.txt\"]]\n",
    "            \n",
    "            for file_path in files:\n",
    "                abs_file_path = child_dir+directory+\"/\"+file_path\n",
    "                record = iterator.iterate(abs_file_path)\n",
    "                json_record = json.dumps(record, ensure_ascii=False)\n",
    "                f.write(json_record + \"\\n\")\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "\n",
    "def run_curation_pipeline(args:Any, jsonl_fp: str) -> str:\n",
    "    \"\"\"\n",
    "    Run the curation pipeline on the dataset.\n",
    "\n",
    "    Args:\n",
    "        args (Any): Command-line arguments.\n",
    "        jsonl_fp (str): The path to the uncurated JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the curated JSONL file.\n",
    "    \"\"\"\n",
    "    client = get_client(args, args.device)\n",
    "    print(f\"    Running the curation pipeline on '{jsonl_fp}'...\")\n",
    "    orig_dataset = DocumentDataset.read_json(jsonl_fp, add_filename=True)\n",
    "    dataset = orig_dataset\n",
    "    #print(type(orig_dataset))\n",
    "    #print(len(orig_dataset.df))\n",
    "    \n",
    "    curation_steps = Sequential(\n",
    "        [\n",
    "            #\n",
    "            # Unify the text encoding to Unicode.\n",
    "            #\n",
    "            Modify(UnicodeReformatter(), text_field=\"title\"),\n",
    "            Modify(UnicodeReformatter(), text_field=\"content\"),\n",
    "            Modify(UnicodeReformatter(), text_field=\"related link\"),\n",
    "            #\n",
    "            nc.Score(\n",
    "                WordCountFilter(min_words=80).score_document,\n",
    "                text_field=\"content\",\n",
    "                score_field=\"word_count\",\n",
    "                score_type=int,\n",
    "            )\n",
    "            \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = curation_steps(dataset)\n",
    "    dataset = dataset.persist()\n",
    "    \n",
    "    print(type(orig_dataset))\n",
    "    print(orig_dataset.df)\n",
    "\n",
    "    print(f\"    Original dataset length: {len(orig_dataset.df)}\")\n",
    "    print(f\"    After running the curation pipeline: {len(dataset.df)}\")\n",
    "    print(f\"    Writing to '{jsonl_fp}'...\")\n",
    "    out_path = os.path.join(\n",
    "        os.path.dirname(jsonl_fp),\n",
    "        \"curated\",\n",
    "    )\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    dataset.to_json(out_path, write_to_filename=True)\n",
    "    client.close()\n",
    "    return os.path.join(out_path, os.path.basename(jsonl_fp))\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser = add_distributed_args(parser)\n",
    "    args = parser.parse_args(args=[])\n",
    "    # Limit the total number of workers to ensure we don't run out of memory.\n",
    "    args.n_workers = min(args.n_workers, 1)\n",
    "\n",
    "    # Prepare the download and JSONL directories.\n",
    "    if not os.path.isdir(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "\n",
    "    jsonl_fp = download_and_convert_to_jsonl()\n",
    "    print(jsonl_fp)\n",
    "    run_curation_pipeline(args, jsonl_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "44a9ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\"\n",
    "DATA_DIR = \"./livedoor_corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "192d3bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download directory:  ./livedoor_corpus\n",
      "Downloading dataset from 'https://www.rondhuit.com/download/ldcc-20140209.tar.gz'...\n",
      "./livedoor_corpus/livedoor.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 33881 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Running the curation pipeline on './livedoor_corpus/livedoor.jsonl'...\n",
      "Reading 1 files\n",
      "<class 'nemo_curator.datasets.doc_dataset.DocumentDataset'>\n",
      "Dask DataFrame Structure:\n",
      "              content                       date filename related link   title     url word_count\n",
      "npartitions=1                                                                                    \n",
      "               object  datetime64[ns, UTC+09:00]   object       object  object  object      int64\n",
      "                  ...                        ...      ...          ...     ...     ...        ...\n",
      "Dask Name: assign, 13 graph layers\n",
      "    Original dataset length: 7367\n",
      "    After running the curation pipeline: 7367\n",
      "    Writing to './livedoor_corpus/livedoor.jsonl'...\n",
      "Writing to disk complete for 1 partitions\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
