{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d3bda8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "from typing import Dict\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from functools import partial\n",
    "from typing import Any\n",
    "\n",
    "import nemo_curator as nc\n",
    "from nemo_curator import ScoreFilter, Sequential\n",
    "from nemo_curator.filters import RepeatingTopNGramsFilter, WordCountFilter\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.modifiers.pii_modifier import PiiModifier\n",
    "from nemo_curator.modifiers.unicode_reformatter import UnicodeReformatter\n",
    "from nemo_curator.modules.modify import Modify\n",
    "from nemo_curator.utils.distributed_utils import get_client\n",
    "from nemo_curator.utils.script_utils import add_distributed_args\n",
    "\n",
    "from nemo_curator.download.doc_builder import (\n",
    "    DocumentDownloader,\n",
    "    DocumentExtractor,\n",
    "    DocumentIterator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "218cf5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalTextDownloader(DocumentDownloader):\n",
    "    def __init__(self, download_dir: str):\n",
    "        super().__init__()\n",
    "\n",
    "        if not os.path.isdir(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "\n",
    "        self._download_dir = download_dir\n",
    "        print(\"Download directory: \", self._download_dir)\n",
    "\n",
    "    def download(self, url: str) -> str:\n",
    "        filename = os.path.basename(url)\n",
    "        filename = re.search(\".+\\.csv\",filename).group()\n",
    "        output_file = os.path.join(self._download_dir, filename)\n",
    "\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"File '{output_file}' already exists, skipping download.\")\n",
    "            return output_file\n",
    "\n",
    "        print(f\"Downloading medical text dataset from '{url}'...\")\n",
    "        response = requests.get(url)\n",
    "\n",
    "        with open(output_file, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3da3fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalTextIterator(DocumentIterator):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._counter = -1\n",
    "\n",
    "\n",
    "    def iterate(self, file_path):\n",
    "        self._counter = -1\n",
    "        file_name = os.path.basename(file_path)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Ignore the first line which contains the header.\n",
    "        file_content = \"\".join(lines[1:])\n",
    "\n",
    "        for line in file_content.split(\"\\n\"):\n",
    "            self._counter += 1\n",
    "            meta = {\n",
    "                \"filename\": file_name,\n",
    "                \"id\": f\"medical_text-{self._counter}\",\n",
    "            }\n",
    "            content = {\"condition label\":line[0:1],\n",
    "                       \"medical abstract\":line[3:-1]}\n",
    "            \n",
    "            #print(content)\n",
    "            \n",
    "            \n",
    "            # Skip if no content extracted\n",
    "            if not content:\n",
    "                continue\n",
    "\n",
    "            record = {**meta, **content}\n",
    "            yield record\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef8068d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./download_text\"\n",
    "DATASET_URL = \"https://huggingface.co/datasets/123rc/medical_text/resolve/main/test.csv?download=true\"\n",
    "\n",
    "def download_and_convert_to_jsonl() -> str:\n",
    "    \"\"\"\n",
    "    Downloads the emails dataset and converts it to JSONL format.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the JSONL file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Download the dataset in raw format and convert it to JSONL.\n",
    "    downloader = MedicalTextDownloader(DATA_DIR)\n",
    "    output_path = os.path.join(DATA_DIR, \"medical_texts.jsonl\")\n",
    "    raw_fp = downloader.download(DATASET_URL)\n",
    "\n",
    "    iterator = MedicalTextIterator()\n",
    "\n",
    "    # Parse the raw data and write it to a JSONL file.\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for record in iterator.iterate(raw_fp):\n",
    "            json_record = json.dumps(record, ensure_ascii=False)\n",
    "            f.write(json_record + \"\\n\")\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0322f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download directory:  ./download_text\n",
      "File './download_text/test.csv' already exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "path = download_and_convert_to_jsonl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1537e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def redact_pii(dataset: DocumentDataset, text_field) -> DocumentDataset:\n",
    "    \"\"\"\n",
    "    Redacts personally identifiable information (PII) from a given dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (DocumentDataset): The dataset containing documents with PII.\n",
    "\n",
    "    Returns:\n",
    "        DocumentDataset: The redacted dataset with PII replaced by a generic value.\n",
    "    \"\"\"\n",
    "    redactor = Modify(\n",
    "        PiiModifier(\n",
    "            supported_entities=[\n",
    "                \"PERSON\",\n",
    "            ],\n",
    "            anonymize_action=\"replace\",\n",
    "            device=\"gpu\",\n",
    "        ),\n",
    "        text_field=text_field,\n",
    "    )\n",
    "    return redactor(dataset)\n",
    "\n",
    "\n",
    "def run_curation_pipeline(args:Any, jsonl_fp: str) -> str:\n",
    "    \"\"\"\n",
    "    Run the curation pipeline on the dataset.\n",
    "\n",
    "    Args:\n",
    "        args (Any): Command-line arguments.\n",
    "        jsonl_fp (str): The path to the uncurated JSONL file.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the curated JSONL file.\n",
    "    \"\"\"\n",
    "    client = get_client(args, args.device)\n",
    "    print(f\"    Running the curation pipeline on '{jsonl_fp}'...\")\n",
    "    orig_dataset = DocumentDataset.read_json(jsonl_fp, add_filename=True)\n",
    "    dataset = orig_dataset\n",
    "\n",
    "    redact_pii_MA = partial(redact_pii, text_field=\"medical abstract\")\n",
    "\n",
    "    curation_steps = Sequential(\n",
    "        [\n",
    "            #\n",
    "            # Unify the text encoding to Unicode.\n",
    "            #\n",
    "            Modify(UnicodeReformatter(), text_field=\"medical abstract\"),\n",
    "            #\n",
    "            # Filtering\n",
    "            #\n",
    "            # Filter out empty emails.\n",
    "            #\n",
    "            # Redact personally identifiable information (PII).\n",
    "            #\n",
    "            redact_pii_MA,\n",
    "            nc.Score(\n",
    "                WordCountFilter(min_words=80).score_document,\n",
    "                text_field=\"medical abstract\",\n",
    "                score_field=\"word_count\",\n",
    "                score_type=int,\n",
    "            )\n",
    "            \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = curation_steps(dataset)\n",
    "    dataset = dataset.persist()\n",
    "\n",
    "    print(f\"    Original dataset length: {len(orig_dataset.df)}\")\n",
    "    print(f\"    After running the curation pipeline: {len(dataset.df)}\")\n",
    "    print(f\"    Writing to '{jsonl_fp}'...\")\n",
    "    out_path = os.path.join(\n",
    "        os.path.dirname(jsonl_fp),\n",
    "        \"curated\",\n",
    "    )\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    dataset.to_json(out_path, write_to_filename=True)\n",
    "    client.close()\n",
    "    return os.path.join(out_path, os.path.basename(jsonl_fp))\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser = add_distributed_args(parser)\n",
    "    args = parser.parse_args(args=[])\n",
    "    # Limit the total number of workers to ensure we don't run out of memory.\n",
    "    args.n_workers = min(args.n_workers, 1)\n",
    "\n",
    "    # Prepare the download and JSONL directories.\n",
    "    if not os.path.isdir(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "\n",
    "    jsonl_fp = download_and_convert_to_jsonl()\n",
    "    run_curation_pipeline(args, jsonl_fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11ab30ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download directory:  ./download_text\n",
      "File './download_text/test.csv' already exists, skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 35989 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Running the curation pipeline on './download_text/medical_texts.jsonl'...\n",
      "Reading 1 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 03:47:01 INFO:Loaded recognizer: EmailRecognizer\n",
      "2024-05-19 03:47:01 INFO:Loaded recognizer: PhoneRecognizer\n",
      "2024-05-19 03:47:01 INFO:Loaded recognizer: EmailRecognizer\n",
      "2024-05-19 03:47:01 INFO:Loaded recognizer: SpacyRecognizer\n",
      "2024-05-19 03:47:01 INFO:Loaded recognizer: PhoneRecognizer\n",
      "2024-05-19 03:47:01 INFO:Loaded recognizer: UsSsnRecognizer\n",
      "2024-05-19 03:47:01 INFO:Loaded recognizer: SpacyRecognizer\n",
      "2024-05-19 03:47:01 INFO:Loaded recognizer: CreditCardRecognizer\n",
      "2024-05-19 03:47:01 INFO:Loaded recognizer: UsSsnRecognizer\n",
      "2024-05-19 03:47:01 INFO:Loaded recognizer: IpRecognizer\n",
      "2024-05-19 03:47:01 INFO:Loaded recognizer: CreditCardRecognizer\n",
      "2024-05-19 03:47:01 WARNING:model_to_presidio_entity_mapping is missing from configuration, using default\n",
      "2024-05-19 03:47:01 INFO:Loaded recognizer: IpRecognizer\n",
      "2024-05-19 03:47:01 WARNING:low_score_entity_names is missing from configuration, using default\n",
      "2024-05-19 03:47:01 WARNING:model_to_presidio_entity_mapping is missing from configuration, using default\n",
      "2024-05-19 03:47:01 WARNING:low_score_entity_names is missing from configuration, using default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Original dataset length: 2889\n",
      "    After running the curation pipeline: 2889\n",
      "    Writing to './download_text/medical_texts.jsonl'...\n",
      "Writing to disk complete for 1 partitions\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96d1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af13d1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
